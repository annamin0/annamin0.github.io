<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- <meta name="description" content="Supervising Sound Localization by In-the-wild Egomotion"> -->
  <!-- <meta name="keywords" content="AI Art, Generative Model, Audio-Visual Learning"> -->
  <!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->
  <title>Supervising Sound Localization by In-the-wild Egomotion</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-7MV5HDD3TQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-7MV5HDD3TQ');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <!-- <link rel="stylesheet" href="./static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <script>
    function setupVideos() {
      for (const video of document.querySelectorAll('.sounding-image')) {
        video.controls = false
        video.addEventListener('mouseover', () => { video.controls = 'controls' })
        video.addEventListener('mouseout', () => { video.controls = false })
      }
    }
    window.addEventListener('load', setupVideos, false)
  </script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-3 publication-title">Supervising Sound Localization by In-the-wild Egomotion</h1>
            <h2 class="title is-5 publication-title"> in submission </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block" style="margin: 0 1rem;">
                <a href="https://annamin0.github.io/" target="_blank">Anna Min</a></span>
              <span class="author-block" style="margin: 0 1rem;">
                <a href="https://ificl.github.io/" target="_blank">Ziyang Chen</a></span>
              <span class="author-block" style="margin: 0 1rem;">
                <a href="https://hangzhaomit.github.io/" target="_blank">Hang Zhao</a></span>
              <span class="author-block" style="margin: 0 1rem;">
                <a href="https://andrewowens.com" target="_blank">Andrew Owens</a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">University of Michigan</span>
            </div>
            
            <div class="is-size-6 publication-authors">
              <span class="author-block">Correspondence to: <span class='rev'>man20@mails.tsinghua.edu.cn</span></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://annamin0.github.io/paper/slinthewild_draft" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                
                <!-- <!- - arXiv Link. - ->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2405.12221" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->

                <!-- Video Link. -->
                
                <!-- <span class="link-block">
                  <a href="#"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video (Coming Soon)</span>
                  </a>
                </span> -->

                <!-- Code Link. -->
                <!-- <span class="link-block">
                  <a href="https://github.com/IFICL/images-that-sound" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
                
                <!-- Bibtex. -->
                <!-- <span class="link-block">
                  <a href="#BibTeX" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-book"></i>
                    </span>
                    <span>BibTex</span>
                  </a>
                </span> -->
              </div>

            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- TEASER + INTRO -->
  <section class="hero teaser" style="margin-top: -30px;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <!-- First column in its own row -->
        <div class="column is-three-fifths">
          <h3 class="subtitle has-text-centered">
            <b>tl;dr:</b> Predicting spatial sound localization using camera ego-motion for in-the-wild videos.
          </h3>
        </div>
      </div>
      <!-- Second column in a separate row -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="is-centered is-max-desktop teaser">
            <div>
              <!-- <video class="center-video" autoplay muted loop playsinline style="margin: 1em auto 0 auto;"> -->
                <img src="./static/images/teaser_one_column.jpg" alt="Image description">
              <!-- </video> -->
              <!-- <p style="font-size: 9pt; width: 75%; margin: 0 auto;">
                Note the above teaser is muted. For examples with sound, please see our <a href="#gallery">gallery</a> below.
              </p> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- OVERVIEW -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>
              We present a method for learning binaural sound localization using egomotion as a supervisory signal. Over the course of a video, the camera's direction to a sound source will change as the camera moves.  We train an audio model to predict sound directions consistent with visual estimates of camera motion, which we obtain using traditional methods from multi-view geometry. This provides a weak but plentiful form of supervision that we combine with traditional binaural cues. To evaluate this method, we propose a dataset of real-world audio-visual videos with egomotion. We show that our model can successfully learn from real-world data and that it performs well on sound localization tasks.
            </p>
            <p>
              We describe our <a href="#method">method</a> in more detail.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>



  <!-- METHOD -->
  <section class="section" style="margin-top: -30px;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered" id="method">Method</h2>
          <div class="content has-text-justified">

              <img src="./static/images/method_1.jpg" class="center-img" style="max-width: 650px; margin-bottom: 30px;"/>

            <p>
              We use a camera's egomotion to 
              supervise binaural sound localization models. We obtain the rota-
              tion and translation of the camera using traditional methods from
              multi-view geometry. We then train an audio model to predict
              sound directions that are consistent with visual camera motions,
              using a dataset that includes in-the-wild walking tours.
            </p>

            <div>
              <img src="./static/images/examples.jpg" class="center-img" style="max-width: 650px; margin-bottom: 30px;"/>
              
              <p style="font-size: 9pt; width: 75%; margin: 0 auto;">
                Visualizations of results in the internet walking tour videos (the YT-Stereo subset). We show our predictions and ground-truth annotations with angles and audio class labels.
              </p>
            </div> 

          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- <section class="section" style="margin-top: -30px;">
    <div class="container is-max-desktop">
      <!-- Concurrent Work. -->
      <!-- <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Related Links and Works</h2>
  
          <div class="content has-text-justified">
            <p>
              Various musicians have inserted images into spectrograms of their music, including 
              <a href="https://www.youtube.com/watch?v=M9xMuPWAZW8" target="_blank">Aphex Twin</a> <span style="font-size:9pt;">(go to 5:27)</span>,
              <a href="https://en.wikipedia.org/wiki/Year_Zero_(album)#Promotion" target="_blank">Nine Inch Nails</a>,
              <a href="https://www.youtube.com/watch?v=BHup81lEjqo" target="_blank">Venetian Snares</a>, and in
              <a href="https://www.youtube.com/watch?v=yzFit0nldf4" target="_blank">Doom's OST</a>. Our work differs 
              from these examples in that our spectrograms both look and sound natural.
            </p>
            <p>
              <a href="https://mixmag.net/feature/spectrogram-art-music-aphex-twin" target="_blank">Spectrogram Art</a>, 
              by <a href="https://www.instagram.com/beckybuckle/" target="_blank">Becky Buckle</a>: an article about the 
              history of artists concealing images in the spectrogram of their music.
            </p>
            <p>
              <a href="https://github.com/LeviBorodenko/spectrographic" target="_blank">SpectroGraphic</a>, 
              by <a href="https://github.com/LeviBorodenko" target="_blank">Levi Borodenko</a>:
              a tool to turn images into spectrograms and recover the corresponding audio.
            </p>
            <p>
              <a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/" target="_blank">Composable Diffusion</a>,
              by <a href="https://nanliu.io/" target="_blank">Liu</a> <i>et al.</i>,
              which originally showed how to compose image diffusion models together.
            </p>
            <p>
              <a href="https://dangeng.github.io/visual_anagrams/" target="_blank">Visual Anagrams</a>,
              by <a href="https://dangeng.github.io/" target="_blank">Geng</a> <i>et al.</i>,
              which uses pretrained diffusion models and compositionality to make multi-view optical illusions.
            </p>
            <p>
              <a href="https://dangeng.github.io/factorized_diffusion/" target="_blank">Factorized Diffusion</a>,
              by <a href="https://dangeng.github.io/" target="_blank">Geng</a> <i>et al.</i>,
              which generates various perceptual illusions via decomposition with diffusion models. We use
              their code the colorize the spectrograms.
            </p>
            <p>
              <a href="https://diffusionillusions.com/" target="_blank">Diffusion Illusions</a>,
              by <a href="https://ryanndagreat.github.io/" target="_blank">Burgert</a> <i>et al.</i>,
              which produces multi-view illusions, along with other visual effects, through score distillation sampling.
              We adapt their code to make an SDS style baseline for generating images that sound.
            </p>
            
          </div>
        </div>
      </div> -->
      <!--/ Concurrent Work. -->
  
    </div>
  </section> -->


  <!-- <section class="section" id="BibTeX" style="margin-top: -30px;">
    <div class="container is-max-desktop content">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title">BibTeX</h2>
          <pre><code>@article{chen2024images,
  title     = {Images that Sound: Composing Images and Sounds on a Single Canvas},
  author    = {Chen, Ziyang and Geng, Daniel and Owens, Andrew},
  journal = {Neural Information Processing Systems (NeurIPS)},
  year      = {2024},
  url       = {https://ificl.github.io/images-that-sound/},
}</code></pre>
        </div>
      </div>
    </div>
  </section> -->


  <footer class="footer">
    <div class="container is-max-desktop">
      <div class="content has-text-centered">
        <!-- <a class="icon-link" href="https://arxiv.org/pdf/2303.11329.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/IFICL" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a> -->
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            
            <p>
              This website is licensed under a 
              <a rel="license" 
              href="http://creativecommons.org/licenses/by-sa/4.0/"
              target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>, 
              and is written by <a href="https://keunhong.com/" target="_blank">
              Keunhong Park</a> for the <a href="https://nerfies.github.io/" 
              target="_blank">Nerfies</a> project. You are free to use the 
              <a href="https://github.com/nerfies/nerfies.github.io"
              target="_blank">source code</a> of this website,
              but please keep these links in the footer, 
              as requested by the authors.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>