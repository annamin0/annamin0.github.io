<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Anna Min</title>

    <meta name="author" content="Anna Min">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Anna Min      <span style="font-family: 'KaiTi', 'STKaiti', serif;">(闵安娜)</span>
                </p>
                <p>Hi! I am an undergraduate at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, who works on multi-modal learning.
                </p>
                <p>
                  I am fortunate to work with Prof. <a href="https://andrewowens.com/">Andrew Owens</a> from <a href="https://umich.edu/">University of Michigan</a> and Prof. <a href="https://hangzhaomit.github.io/">Hang Zhao</a> from <a href="https://iiis.tsinghua.edu.cn/en/">Tsinghua University</a>.
                </p>
                <p style="color:rgb(255, 0, 21); font-weight: bold;">I am actively applying for a Ph.D. position starting from 2024 Fall!</p>
                <p style="text-align:center">
                  <a href="mailto:anna.min1754@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/info/annamin-20231203-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/_AnnaMin">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yangqing-20/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:20%;max-width:20%">
                <a href="images/info/AnnaMin.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/info/AnnaMin.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am broadly interested in creating machine learning models that combine multiple modalities to perceive and engage with the world.
                  <br><br>
                  Currently, I work on embodied AI, audio-visual learning and acoustics interactive modeling. I am also interested in multi-modal generative models(audio, images, music, text, etc.) and open to other topics.
                  <br><br>
                  Previously, I have worked on combining vision and sound in
                  <ol>
                    <li>spatial audio analysis, which is crucial for embodied AI in vision-challenged scenarios and contributes to creating 360-degree auditorily immersive environments and interactive music generation</li>
                    <br>
                    <li>emotional voice generation, for effective cross-lingual communication, affective computing and human-robot interaction from non-verbal multi-modal information</li>
                  </ol>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr onmouseout="camp_stop()" onmouseover="camp_start()">
              <td style="padding:20px;width:45%;vertical-align:middle">
                <div class="one">
                  <!-- <div class="two" id='camp_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/camp.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div> -->
                  <img src='images/paper/slinthewild.jpg' width="350">
                </div>
              </td>
              <td style="padding:20px;width:55%;vertical-align:middle">
                <a>
                  <span class="papertitle">Crafting Precision in the Wild: Learning Spatial Sound Localization from Unlabelled Egocentric Videos</span>
                </a>
                <br>
                <strong>Anna Min</strong>,
                <a href="https://ificl.github.io/">Ziyang Chen</a>,
                <a href="https://hangzhaomit.github.io/">Hang Zhao</a>,
                <a href="https://andrewowens.com/">Andrew Owens</a>
                <br>
                <!-- <em>AAAI24 Workshop on Digital Human, Oral Presentation</em>
                <br> -->
                <em>in submission</em>
                <br>
                <p></p>
                <p>
                  Explore weak labels based on the direction of source motion for accurate binaural sound localization prediction and introduce the first stereo dataset and benchmark for sound localization in the wild.
                </p>
              </td>
            </tr>

            <tr onmouseout="camp_stop()" onmouseover="camp_start()">
              <td style="padding:20px;width:45%;vertical-align:middle">
                <div class="one">
                  <!-- <div class="two" id='camp_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/camp.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div> -->
                  <img src='images/paper/emotiontransfer.jpg' width="350">
                </div>
              </td>
              <td style="padding:20px;width:55%;vertical-align:middle">
                <a>
                  <span class="papertitle">Fine-grained Emotion Transfer for Speech-to-Speech Translation in Expressive Video Dubbing</span>
                </a>
                <br>
                <strong>Anna Min</strong>,
                <a href="https://huchenxucs.github.io/">Chenxu Hu</a>,
                <a href="https://rayeren.github.io/">Yi Ren</a>,
                <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
                <br>
                <em>AAAI24 Workshop on Digital Human, Oral Presentation</em>
                <br>
                <em>in submission to NAACL2024</em>
                <br>
                <p></p>
                <p>
                  Construct a newly constructed dataset with aligned bilingual audio tracks and use wav to unit translation and unit to wav HiFi-GAN-based networks for transferring pitches and rhythms.
                </p>
              </td>
            </tr>


    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:45%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='camp_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/camp.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <img src='images/paper/cascaded.jpg' width="350">
        </div>
      </td>
      <td style="padding:20px;width:55%;vertical-align:middle">
        <a>
          <span class="papertitle">When End-to-End is Overkill: Rethinking Cascaded Speech-to-Text Translation</span>
        </a>
        <br>
        <strong>Anna Min</strong>,
        <a href="https://huchenxucs.github.io/">Chenxu Hu</a>,
        <a href="https://rayeren.github.io/">Yi Ren</a>,
        <a href="https://scholar.google.com/citations?user=e6_J-lEAAAAJ&hl=en">Xiang Yin</a>,
        <a href="https://hangzhaomit.github.io/">Hang Zhao</a>
        <br>
        <em>in submission to COLING2024</em>
        <br>
        <p></p>
        <p>
          Propose a method that leverages two distinct pretrained autoregressive models to enhance a cascaded system and address the challenges associated with error divergence.
        </p>
      </td>
    </tr>


          </tbody></table>

          <br><br>
          <br><br>

          <!-- Experience -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px;width:100%;vertical-align:middle">
                  <h2>Experience<h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px 50px;width:25%;vertical-align:middle" align="center">
                  <img src="images\favicon\UMich_logo.png" style="border-style: none" height="100">
                </td>

                <td width="75%" valign="middle">
                  <university>University of Michigan (Ann Arbor, U.S.A.)</university>
                  <br>
                  2023.07 ~ Now
                  <br>
                  Visiting Research Intern, Multi-modal Learning
                  <br>
                  Advisor: Prof. <a href="https://andrewowens.com/">Andrew Owens</a> 
                </td>
              </tr>

              <tr>
                <td style="padding:10px 50px;width:25%;vertical-align:middle" align="center">
                  <img src="images\favicon\Tsinghua_logo.png" style="border-style: none" height="100">
                </td>

                <td width="75%" valign="middle">
                  <!-- heading -->
                  <university>Tsinghua University (Beijing, China)</university>
                  <br>
                  2022.10 ~ Now
                  <br>
                  Research Intern, Multi-modal Learning
                  <br>
                  Advisor: Prof. <a href="https://hangzhaomit.github.io/">Hang Zhao</a> 
                  <!-- conference & date -->
                </td>
              </tr>

            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Awards</h2>
                <ul>
                  <li>Tsinghua University Research Excellence Award (2/103), 2023</li>
                  <li>Tsinghua University Academic Excellence Award (2/103), 2023</li>
                  <li>Spark Innovative Talent Cultivation Program (50/3900 undergraduates in Tsinghua for research performance), 2022</li>
                  <li>Tsinghua University Research Excellence Award (2/103), 2022</li>
                </ul>
              </td>

            </tr>
          </tbody>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
                <p>
                  I am an amateur illustrator(currently fascinated by generative models though).<br>
                  My Chinese name is pronounced as Ān Nà, which is similar to Anna.
                </p>
              </td>

            </tr>
          </tbody>
        
        </table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>


            
<tr>
  <td align="right">
    <p>
      Last updated Dec. 2023.
    </p>
    <p>
      Template from <a href="https://jonbarron.info/">Jonathan Barron</a>.
    </p>
  </td>
</tr>
          
            
            
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
